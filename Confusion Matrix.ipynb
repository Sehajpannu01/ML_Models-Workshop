{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291e040b-4c43-449c-a4bd-5396fadffcf2",
   "metadata": {},
   "source": [
    "### Confusion Matrix:- \n",
    "[Table showing the performance of a classification model by comparing actual vs predicted outcomes]\n",
    "* True Positives (TP): Correctly predicted positive cases.\n",
    "* True Negatives (TN): Correctly predicted negative cases.\n",
    "* False Positives (FP): Incorrectly predicted positive cases (Type I error).\n",
    "*  False Negatives (FN): Incorrectly predicted negative cases (Type II error).\n",
    "\n",
    "\n",
    "- Prescision Score:- [Model Purity]\n",
    "- Recall Score:- [Model Sensitivity]\n",
    "- F1-Score:- [Harmonic mean of Precision and Recall]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c5ca4-c2a9-407b-8ece-c7c5f97d985e",
   "metadata": {},
   "source": [
    "### Evaluating Classifiers: Using Confusion Matrix, Precision, Recall & F1-Score\n",
    "\n",
    "#### Why is Accuracy Not Always a Good Metric to Evaluate Classifiers?\n",
    "> Accuracy is a common metric used to evaluate classifiers and provides the ratio of correctly predicted instances to the total instances.\n",
    "- However, accuracy may not be reliable in cases of imbalanced or skewed datasets (where one class dominates over the others). In such cases, a high accuracy can be misleading because the model might simply predict the majority class most of the time and still achieve high accuracy, while failing to correctly classify the minority class.\n",
    "\n",
    "* For example, in a dataset where 95% of instances belong to one class, a model that predicts the majority class 100% of the time will achieve 95% accuracy, despite being useless at identifying the minority class. Metrics like precision, recall, F1-score, and others are more appropriate for such cases.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d615ffcd-e6ac-456b-8e23-9dc7371d9e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
